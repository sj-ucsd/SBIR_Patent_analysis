{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "400d90f0-a648-4559-9f6e-e6319cc6cdee",
   "metadata": {},
   "source": [
    "# Spacy helper functions\n",
    "Helper functions for Spacy such as tokenizer, lemmatizer, entity extraction etc. Note that there is spacy and there is scispacy. Scispacy seems to be faster but doesn't seem to provide the subcategories of entities like spacy does. We may use a combination of the two. Also note that scispacy is trained with an older version of spacy (3.6.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd72e468-ef01-4226-bc15-1d7f08ae1652",
   "metadata": {},
   "source": [
    "### Usage\n",
    "1. Install import-ipynb module (!pip install import-ipynb)\n",
    "2. In the notebook to use these helper functions, add the following code:\n",
    "- import import_ipynb\n",
    "- import spacy_helper_methods as sphelpers\n",
    "\n",
    "You can then access the methods defined here. For example\n",
    "sphelpers.lemmatize(....)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e45fffe-c346-4889-9aa4-bfafc05ff71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These files are needed for this notebook. If not installed, uncomment the lines and run the installation scripts\n",
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "533be28e-95d1-49f5-91b8-6636034a5b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy as sp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cee744fe-8b11-43d7-9bf8-cbe76c5f2823",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lemmatize. Takes a Pandas Series as input and returns a lemmatized pandas series\n",
    "def lemmatize(nlpname, ds):\n",
    "    lemma_ds = ds.apply(lambda x: \" \".join(token.lemma_ for token in nlpname(x) \n",
    "                                     if token.lemma_.lower() not in nlpname.Defaults.stop_words))\n",
    "                                     \n",
    "    return lemma_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6f02c04-e301-40ad-8a49-d9a4c466526e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get named entity recognition\n",
    "def get_entities(nlpname, ds):\n",
    "    ent_ds = ds.apply(lambda x: [(ent.text, ent.label_) for ent in nlpname(x).ents])\n",
    "    return ent_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ea02b28-05bb-45b3-b33d-c1c1de47c056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add additional helper functions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f744b03-b060-4872-acbf-98f07d51e6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_non_tech(x, model):\n",
    "    if x:\n",
    "        y_pred = model.predict(x)\n",
    "    return(list(set([x[i] for i in range(len(x)) if y_pred[i]])))\n",
    "\n",
    "def extract_tech_entities(nlp, model, ds):\n",
    "    lemma_ds = lemmatize(nlp, ds)\n",
    "    ent_ds = get_entities(nlp,lemma_ds)\n",
    "    ent_ds = ent_ds.apply(lambda x: [ent[0] for ent in x])\n",
    "    ent_ds = ent_ds.apply(lambda x: filter_non_tech(x, model))\n",
    "    return ent_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442bbe3b-a731-48a7-92c0-3e15fcd9e364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
