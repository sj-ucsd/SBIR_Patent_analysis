{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4c784db-4784-4d68-91b0-4bc7aefda5db",
   "metadata": {},
   "source": [
    "# Extract Non-technical terms\n",
    "Use a sample of 10k SBIR documents to extract non-technical terms. We basically do entity extraction from the 10k samples here. The SBIR document is in csv format and can be downlaoded from: https://www.sbir.gov/sbirsearch/award/all - We used \"with abstract data\" option to download. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2ce4756-dcba-4e27-b122-ee00cf64fa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If these modules or packages are not installed, then uncomment the following lines and install them\n",
    "\n",
    "#!pip install import-ipynb \n",
    "#!pip install spacy\n",
    "#alternative installation method may work if the above version does not\n",
    "#!pip install -U spacy\n",
    "\n",
    "#!python -m spacy download en_core_web_lg to be installed\n",
    "#If the above installation method does not work, the following command might work\n",
    "#!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.0.0/en_core_web_lg-3.0.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bad0fa1-8368-4214-8ecf-86a36807860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain\n",
    "import import_ipynb\n",
    "import spacy as sp\n",
    "import nltk\n",
    "import spacy\n",
    "import requests\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dbf5292-8aa2-46ad-9c57-d47eb60e794e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from spacy_helper_methods.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Import spacy_helper_methods notebook should be in same directory\n",
    "import spacy_helper_methods as sph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7444ca-81b8-4864-b048-586b7205efb0",
   "metadata": {},
   "source": [
    "## Load, Preprocess, cleanup and sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d4c93b0-7327-478b-bba3-f47c9a10fe8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.02 s, sys: 1.07 s, total: 3.09 s\n",
      "Wall time: 8.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# read SBIR award data directly from web URL\n",
    "url=\"https://data.www.sbir.gov/awarddatapublic/award_data.csv\"\n",
    "s=requests.get(url).content\n",
    "sbir_df=pd.read_csv(io.StringIO(s.decode('utf-8')),usecols=['Abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b432f77-2e22-4468-ace3-86e8911ed3ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173500"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleanup\n",
    "sbir_df = sbir_df.dropna()\n",
    "len(sbir_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d55a07a0-00f7-41a4-a673-bcbd1cd916a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a sample of 10k abstracts to create non-tech words \n",
    "sbir_df = sbir_df.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95ec8530-9ffe-49aa-b46e-75ac73e76039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72746</th>\n",
       "      <td>DESCRIPTION (provided by applicant): Methadone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58821</th>\n",
       "      <td>This proposed study intends to develop a model...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94023</th>\n",
       "      <td>The current and next generation military jet a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52540</th>\n",
       "      <td>This Small Business Innovation Research (SBIR)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66431</th>\n",
       "      <td>We aim to solve a technical problem that is hi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Abstract\n",
       "72746  DESCRIPTION (provided by applicant): Methadone...\n",
       "58821  This proposed study intends to develop a model...\n",
       "94023  The current and next generation military jet a...\n",
       "52540  This Small Business Innovation Research (SBIR)...\n",
       "66431  We aim to solve a technical problem that is hi..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbir_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff0b9e29-a175-42af-b231-ecd0dac157fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/laben/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords using nltk. Initial parse seems to be faster than spacy and \n",
    "# doesn't seem to remove all stopwords compared to spacy hence after inital pass\n",
    "# will still filter on spacy's stopwords in lemmatize method\n",
    "\n",
    "#download stopwords (will cleaning skip if its already downloaded)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7966ed2e-c7d7-4e70-9408-e965b3279f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.61 s, sys: 5.27 ms, total: 1.61 s\n",
      "Wall time: 1.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sbir_df = sbir_df['Abstract'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f96b42-dddc-49aa-a3a1-c2aa2c5c2191",
   "metadata": {},
   "source": [
    "## Extract entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdaa2896-681c-42dc-90df-e24026ecf21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the scispacy large vocabulary - gives us lot more entities than spacy \n",
    "# note that sci spacy vocabulary was trained with spacy 3.6.1 hence may get warning\n",
    "\n",
    "nlp = sp.load(\"en_core_web_lg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9777ef6-bf7c-45bf-9db7-683baa778ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 21s, sys: 431 ms, total: 4min 21s\n",
      "Wall time: 4min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load the scispacy large vocabulary. Lemmatization seems to be faster with this \n",
    "lemma_ds = sph.lemmatize(nlp, sbir_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d41df4a-5d6d-43e5-9d91-72527f4ce003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 22s, sys: 24.6 ms, total: 4min 22s\n",
      "Wall time: 4min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ent_ds = sph.get_entities(nlp, lemma_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dc9964-c7d7-43ba-90c3-b469dc022aca",
   "metadata": {},
   "source": [
    "## Filter entities \n",
    "1. Remove entities such as time, money, quantity\n",
    "2. remove any entities that match technical terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89a4b02c-d540-4250-af62-18225ce069e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse all tuples into a flat list\n",
    "tuple_list = list(set(list(chain.from_iterable(ent_ds.values))))\n",
    "tuple_list = [tuple for tuple in tuple_list if tuple[1] not in ['CARDINAL','PERCENT','TIME','QUANTITY','MONEY']]\n",
    "entities = list(set(list([i[0] for i in tuple_list])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c788a9c2-1993-4275-95fc-15815d469557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any entities that are in the technical word list\n",
    "# load technical terms\n",
    "with open('../preprocessed_files/tech_terms.txt','r') as f:\n",
    "    text = f.read()\n",
    "techwords = text.split('\\n')\n",
    "ts = pd.Series(techwords).str.lower()\n",
    "ts.head()\n",
    "\n",
    "#convert entity list generated above from SBIR abstract to lower\n",
    "nts = pd.Series(entities).str.lower()\n",
    "\n",
    "#filter all technical words from nts\n",
    "nts = nts[~nts.isin(ts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f22a132d-6c72-4ed9-a009-28837b7be653",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = [entity for entity in entities if entity.lower() in list(nts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9439b60c-1e35-4f64-aa4a-5125c826a7cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29894,\n",
       " ['Ron Hatch', 'Nu - Trek', 'Innoveering', 'schlieren', 'ECL'],\n",
       " [('coatingswill', 'ORG'),\n",
       "  ('Electron - Ion Colliders', 'ORG'),\n",
       "  ('USMC', 'ORG'),\n",
       "  ('TYPICAL', 'ORG'),\n",
       "  ('Egret', 'ORG')])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entities), entities[:5], tuple_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40407f75-d0b1-47f9-89e2-c286d446636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../preprocessed_files/non_tech.txt','w') as nf:\n",
    "    nf.writelines('\\n'.join(map(str, entities)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60129353-47a5-44eb-8b53-a56f35d7b2e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
