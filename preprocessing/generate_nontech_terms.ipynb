{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4c784db-4784-4d68-91b0-4bc7aefda5db",
   "metadata": {},
   "source": [
    "# Extract Non-technical terms\n",
    "Use a sample of 10k SBIR documents to extract non-technical terms. We basically do entity extraction from the 10k samples here. The SBIR document is in csv format and can be downlaoded from: https://www.sbir.gov/sbirsearch/award/all - We used \"with abstract data\" option to download. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2ce4756-dcba-4e27-b122-ee00cf64fa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install import-ipynb \n",
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bad0fa1-8368-4214-8ecf-86a36807860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain\n",
    "import import_ipynb\n",
    "import spacy as sp\n",
    "import nltk\n",
    "import spacy\n",
    "import requests\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dbf5292-8aa2-46ad-9c57-d47eb60e794e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from spacy_helper_methods.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Import spacy helper functions from the helper notebook\n",
    "\n",
    "#subdirectory = \"/Users/prakhar/Documents/work/ucsd/dse203/SBIR_Patent_analysis\"\n",
    "\n",
    "# PS spacy_helper_methods notebook should be in same directory\n",
    "\n",
    "import spacy_helper_methods as sph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7444ca-81b8-4864-b048-586b7205efb0",
   "metadata": {},
   "source": [
    "## Load, Preprocess, cleanup and sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d4c93b0-7327-478b-bba3-f47c9a10fe8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.66 s, sys: 10.3 s, total: 18.9 s\n",
      "Wall time: 50.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# read SBIR award data directly from web URL\n",
    "url=\"https://data.www.sbir.gov/awarddatapublic/award_data.csv\"\n",
    "s=requests.get(url).content\n",
    "sbir_df=pd.read_csv(io.StringIO(s.decode('utf-8')),usecols=['Abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b432f77-2e22-4468-ace3-86e8911ed3ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173500"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleanup\n",
    "sbir_df = sbir_df.dropna()\n",
    "len(sbir_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d55a07a0-00f7-41a4-a673-bcbd1cd916a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a sample of 10k abstracts to create non-tech words \n",
    "sbir_df = sbir_df.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95ec8530-9ffe-49aa-b46e-75ac73e76039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90144</th>\n",
       "      <td>This Small Business Technology Transfer Resear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65438</th>\n",
       "      <td>DESCRIPTION (provided by applicant): Seasonal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46240</th>\n",
       "      <td>Phase II of the New METSAT Display project wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184876</th>\n",
       "      <td>THE LONG-TERM GOAL OF THIS RESEARCH IS TO DEVE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83843</th>\n",
       "      <td>DESCRIPTION (provided by applicant): Indolamin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Abstract\n",
       "90144   This Small Business Technology Transfer Resear...\n",
       "65438   DESCRIPTION (provided by applicant): Seasonal ...\n",
       "46240   Phase II of the New METSAT Display project wil...\n",
       "184876  THE LONG-TERM GOAL OF THIS RESEARCH IS TO DEVE...\n",
       "83843   DESCRIPTION (provided by applicant): Indolamin..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbir_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff0b9e29-a175-42af-b231-ecd0dac157fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sagarjogadhenu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords using nltk. Initial parse seems to be faster than spacy and \n",
    "# doesn't seem to remove all stopwords compared to spacy hence after inital pass\n",
    "# will still filter on spacy's stopwords in lemmatize method\n",
    "\n",
    "nltk.download('stopwords')    # PS needs to be downloaded\n",
    "\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7966ed2e-c7d7-4e70-9408-e965b3279f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.17 s, sys: 149 ms, total: 7.32 s\n",
      "Wall time: 8.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sbir_df = sbir_df['Abstract'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f96b42-dddc-49aa-a3a1-c2aa2c5c2191",
   "metadata": {},
   "source": [
    "## Extract entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07c3cdfa-ae6a-4196-9766-60394da19e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdaa2896-681c-42dc-90df-e24026ecf21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the scispacy large vocabulary - gives us lot more entities than spacy \n",
    "# note that sci spacy vocabulary was trained with spacy 3.6.1 hence may get warning\n",
    "\n",
    "nlp = sp.load(\"en_core_web_lg\")  #PS !python -m spacy download en_core_web_lg to be installed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9777ef6-bf7c-45bf-9db7-683baa778ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 5s, sys: 46.6 s, total: 7min 52s\n",
      "Wall time: 9min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load the scispacy large vocabulary. Lemmatization seems to be faster with this \n",
    "lemma_ds = sph.lemmatize(nlp, sbir_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d41df4a-5d6d-43e5-9d91-72527f4ce003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 10s, sys: 43.5 s, total: 6min 53s\n",
      "Wall time: 10min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ent_ds = sph.get_entities(nlp, lemma_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dc9964-c7d7-43ba-90c3-b469dc022aca",
   "metadata": {},
   "source": [
    "## Filter entities \n",
    "1. Remove entities such as time, money, quantity\n",
    "2. remove any entities that match technical terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89a4b02c-d540-4250-af62-18225ce069e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse all tuples into a flat list\n",
    "tuple_list = list(set(list(chain.from_iterable(ent_ds.values))))\n",
    "tuple_list = [tuple for tuple in tuple_list if tuple[1] not in ['CARDINAL','PERCENT','TIME','QUANTITY','MONEY']]\n",
    "entities = list(set(list([i[0] for i in tuple_list])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c788a9c2-1993-4275-95fc-15815d469557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any entities that are in the technical word list\n",
    "# load technical terms\n",
    "with open('../preprocessed_files/tech_terms.txt','r') as f:\n",
    "    text = f.read()\n",
    "techwords = text.split('\\n')\n",
    "ts = pd.Series(techwords).str.lower()\n",
    "ts.head()\n",
    "\n",
    "#convert entity list generated above from SBIR abstract to lower\n",
    "nts = pd.Series(entities).str.lower()\n",
    "\n",
    "#filter all technical words from nts\n",
    "nts = nts[~nts.isin(ts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f22a132d-6c72-4ed9-a009-28837b7be653",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = [entity for entity in entities if entity.lower() in list(nts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9439b60c-1e35-4f64-aa4a-5125c826a7cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29826,\n",
       " ['CHOLECYSTOKININ',\n",
       "  'lacritin efficacious',\n",
       "  'c / c',\n",
       "  'Robustness UAS Technology',\n",
       "  'WisdomTools Project Lead Way'],\n",
       " [('projectis', 'NORP'),\n",
       "  ('Omniox', 'ORG'),\n",
       "  ('F - NVS', 'PRODUCT'),\n",
       "  ('R5 X4', 'PRODUCT'),\n",
       "  ('PDT DRUG', 'ORG')])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entities), entities[:5], tuple_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40407f75-d0b1-47f9-89e2-c286d446636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../preprocessed_files/non_tech.txt','w') as nf:\n",
    "    nf.writelines('\\n'.join(map(str, entities)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddb6498-d52f-46e4-834e-4ebcebf18732",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
